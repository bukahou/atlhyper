// =======================================================================================
// ğŸ“„ overview.goï¼ˆinternal/query/clusterï¼‰
//
// âœ¨ æ–‡ä»¶åŠŸèƒ½è¯´æ˜ï¼š
//
//	æä¾› Kubernetes é›†ç¾¤çš„æ•´ä½“æ¦‚è¦ä¿¡æ¯ï¼Œç”¨äºé›†ç¾¤é¦–é¡µå±•ç¤ºã€‚
//	åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š
//	- èŠ‚ç‚¹æ€»æ•°ä¸ Ready èŠ‚ç‚¹æ•°
//	- æ‰€æœ‰å‘½åç©ºé—´ä¸‹çš„ Pod æ€»æ•°
//	- å¼‚å¸¸çŠ¶æ€çš„ Pod æ•°é‡ï¼ˆPending / Failed / Unknownï¼‰
//	- Kubernetes æ§åˆ¶å¹³é¢ç‰ˆæœ¬
//	- æ˜¯å¦éƒ¨ç½²äº† metrics-serverï¼ˆåˆ¤æ–­æ˜¯å¦æ”¯æŒ CPU/Mem æŸ¥è¯¢ï¼‰
//
// ğŸ“¦ å¤–éƒ¨ä¾èµ–ï¼š
//   - utils.GetCoreClient()ï¼ˆå°è£…çš„ client-go å®¢æˆ·ç«¯ï¼‰
//   - utils.HasMetricsServer()ï¼ˆæ£€æµ‹ metrics-server æ˜¯å¦å¯ç”¨ï¼‰
//
// âœï¸ ä½œè€…ï¼šbukahou (@ZGMF-X10A)
// ğŸ“… åˆ›å»ºæ—¶é—´ï¼š2025å¹´7æœˆ
// =======================================================================================
package cluster

import (
	"NeuroController/internal/utils"
	"NeuroController/model/clusteroverview"
	"context"
	"fmt"
	"strings"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	metricsclient "k8s.io/metrics/pkg/client/clientset/versioned"
)

func GetClusterOverview(ctx context.Context) (*clusteroverview.ClusterOverview, error) {
	client := utils.GetCoreClient()

	// 1) Nodesï¼šæ€»æ•°ã€Ready æ•°ã€ç´¯è®¡ allocatableï¼ˆæ€»é‡ï¼‰ï¼ŒåŒæ—¶å‡†å¤‡ per-node æ€»é‡
	nodes, err := client.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•è·å– Node åˆ—è¡¨: %w", err)
	}
	totalNodes := len(nodes.Items)
	readyNodes := 0

	var totalCPUMilli int64
	var totalMemBytes int64

	// é€èŠ‚ç‚¹æ€»é‡ï¼ˆallocatableï¼‰
	type nodeCap struct {
		cpuMilli int64
		memBytes int64
		ready    bool
		role     string
	}
	perNodeCap := make(map[string]nodeCap, totalNodes)

	for _, node := range nodes.Items {
		ready := false
		for _, cond := range node.Status.Conditions {
			if cond.Type == corev1.NodeReady && cond.Status == corev1.ConditionTrue {
				ready = true
				break
			}
		}
		if ready {
			readyNodes++
		}

		var cpuMilli int64
		var memBytes int64
		if cpuQty, ok := node.Status.Allocatable[corev1.ResourceCPU]; ok {
			cpuMilli = cpuQty.MilliValue()
			totalCPUMilli += cpuMilli
		}
		if memQty, ok := node.Status.Allocatable[corev1.ResourceMemory]; ok {
			memBytes = memQty.Value()
			totalMemBytes += memBytes
		}

		// ç®€å•è§£æ roleï¼ˆå¯é€‰ï¼‰
		role := ""
		for k := range node.Labels {
			if strings.HasPrefix(k, "node-role.kubernetes.io/") {
				role = strings.TrimPrefix(k, "node-role.kubernetes.io/")
				if role == "" {
					role = "master"
				}
				break
			}
		}

		perNodeCap[node.Name] = nodeCap{
			cpuMilli: cpuMilli,
			memBytes: memBytes,
			ready:    ready,
			role:     role,
		}
	}

	// 2) Podsï¼šæ€»æ•°ã€å¼‚å¸¸æ•°
	pods, err := client.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•è·å– Pod åˆ—è¡¨: %w", err)
	}
	totalPods := len(pods.Items)
	abnormalPods := 0
	for _, pod := range pods.Items {
		switch pod.Status.Phase {
		case corev1.PodFailed, corev1.PodPending, corev1.PodUnknown:
			abnormalPods++
		}
	}

	// 3) ç‰ˆæœ¬
	versionInfo, err := client.Discovery().ServerVersion()
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•è·å– Kubernetes ç‰ˆæœ¬: %w", err)
	}

	ov := &clusteroverview.ClusterOverview{
		TotalNodes:   totalNodes,
		ReadyNodes:   readyNodes,
		TotalPods:    totalPods,
		AbnormalPods: abnormalPods,
		K8sVersion:   versionInfo.GitVersion,
		HasMetrics:   utils.HasMetricsServer(),
	}

	// 4) è‹¥ metrics-server å¯ç”¨ï¼šæ±‡æ€»æ€»ä½¿ç”¨ + é€èŠ‚ç‚¹ä½¿ç”¨
	if ov.HasMetrics {
		restCfg := utils.GetRestConfig()
		mc, err := metricsclient.NewForConfig(restCfg)
		if err == nil {
			nmList, err := mc.MetricsV1beta1().NodeMetricses().List(ctx, metav1.ListOptions{})
			if err == nil {
				var usedCPUMilli int64
				var usedMemBytes int64

				// å…ˆæŠŠ per-node used æ”¾è¿› map
				type nodeUsed struct {
					cpuMilli int64
					memBytes int64
				}
				perNodeUsed := make(map[string]nodeUsed, len(nmList.Items))
				for _, m := range nmList.Items {
					perNodeUsed[m.Name] = nodeUsed{
						cpuMilli: m.Usage.Cpu().MilliValue(),
						memBytes: m.Usage.Memory().Value(),
					}
				}

				// æ±‡æ€»æ€»ä½¿ç”¨
				for _, u := range perNodeUsed {
					usedCPUMilli += u.cpuMilli
					usedMemBytes += u.memBytes
				}

				// æ€»èµ„æºæ±‡æ€»ï¼ˆcluster levelï¼‰
				res := &clusteroverview.ClusterResourceSummary{
					TotalCPUMilli:    totalCPUMilli,
					UsedCPUMilli:     usedCPUMilli,
					TotalCPUCores:    float64(totalCPUMilli) / 1000.0,
					UsedCPUCores:     float64(usedCPUMilli) / 1000.0,
					TotalMemoryBytes: totalMemBytes,
					UsedMemoryBytes:  usedMemBytes,
				}
				if totalCPUMilli > 0 {
					res.CPUPercent = (float64(usedCPUMilli) / float64(totalCPUMilli)) * 100.0
				}
				if totalMemBytes > 0 {
					res.MemoryPercent = (float64(usedMemBytes) / float64(totalMemBytes)) * 100.0
				}
				ov.Resources = res

				// é€èŠ‚ç‚¹ç»„è£…
				ov.Nodes = make([]clusteroverview.NodeResourceUsage, 0, len(perNodeCap))
				for name, cap := range perNodeCap {
					u := perNodeUsed[name] // è‹¥ metrics-server é‡Œæ²¡æœ‰è¯¥èŠ‚ç‚¹ï¼Œä¼šæ˜¯é›¶å€¼
					nodeItem := clusteroverview.NodeResourceUsage{
						NodeName:         name,
						TotalCPUMilli:    cap.cpuMilli,
						UsedCPUMilli:     u.cpuMilli,
						TotalCores:       float64(cap.cpuMilli) / 1000.0,
						UsedCores:        float64(u.cpuMilli) / 1000.0,
						TotalMemoryBytes: cap.memBytes,
						UsedMemoryBytes:  u.memBytes,
						Ready:            cap.ready,
						Role:             cap.role,
					}
					if cap.cpuMilli > 0 {
						nodeItem.CPUPercent = (float64(u.cpuMilli) / float64(cap.cpuMilli)) * 100.0
					}
					if cap.memBytes > 0 {
						nodeItem.MemoryPercent = (float64(u.memBytes) / float64(cap.memBytes)) * 100.0
					}
					ov.Nodes = append(ov.Nodes, nodeItem)
				}
			}
			// å¦‚æœ metrics API å¤±è´¥ï¼šä¿æŒ ov.Resources=nilï¼Œov.Nodes ä¹Ÿä¸å¡« used/percent
		}
	}

	// å¦‚æœæ²¡æœ‰ metrics-serverï¼šä»è¿”å›åŸºç¡€æ¦‚è§ˆå’Œ perNode æ€»é‡ï¼ˆå¯é€‰ï¼‰
	// å¦‚éœ€åœ¨æ—  metrics-server æ—¶ä¹Ÿè¿”å› per-node æ€»é‡ï¼Œå¯åœ¨æ­¤å¤„å¡«å…… ov.Nodes ä½† used=0ã€percent=0ã€‚
	if !ov.HasMetrics && len(ov.Nodes) == 0 {
		ov.Nodes = make([]clusteroverview.NodeResourceUsage, 0, len(perNodeCap))
		for name, cap := range perNodeCap {
			ov.Nodes = append(ov.Nodes, clusteroverview.NodeResourceUsage{
				NodeName:         name,
				TotalCPUMilli:    cap.cpuMilli,
				UsedCPUMilli:     0,
				CPUPercent:       0,
				TotalCores:       float64(cap.cpuMilli) / 1000.0,
				UsedCores:        0,
				TotalMemoryBytes: cap.memBytes,
				UsedMemoryBytes:  0,
				MemoryPercent:    0,
				Ready:            cap.ready,
				Role:             cap.role,
			})
		}
	}

	return ov, nil
}






// // ClusterOverview å®šä¹‰äº†é›†ç¾¤æ¦‚è¦æ•°æ®çš„ç»“æ„ï¼Œç”¨äºé¦–é¡µ UI å±•ç¤º
// type ClusterOverview struct {
// 	TotalNodes   int    `json:"total_nodes"`        // èŠ‚ç‚¹æ€»æ•°
// 	ReadyNodes   int    `json:"ready_nodes"`        // Ready çŠ¶æ€çš„èŠ‚ç‚¹æ•°
// 	TotalPods    int    `json:"total_pods"`         // æ‰€æœ‰å‘½åç©ºé—´ä¸­çš„ Pod æ€»æ•°
// 	AbnormalPods int    `json:"abnormal_pods"`      // å¼‚å¸¸çŠ¶æ€ï¼ˆPending/Failed/Unknownï¼‰çš„ Pod æ•°é‡
// 	K8sVersion   string `json:"k8s_version"`        // Kubernetes æ§åˆ¶å¹³é¢ç‰ˆæœ¬
// 	HasMetrics   bool   `json:"has_metrics_server"` // æ˜¯å¦æ£€æµ‹åˆ° metrics-server
// }

// // GetClusterOverview è¿”å›å½“å‰é›†ç¾¤çš„æ¦‚è¦ä¿¡æ¯
// // âœ… ç”¨äºé¦–é¡µé›†ç¾¤æ¦‚è§ˆæ¥å£ï¼Œå¦‚ï¼šGET /api/cluster/overview
// func GetClusterOverview(ctx context.Context) (*ClusterOverview, error) {
// 	client := utils.GetCoreClient()

// 	// 1ï¸âƒ£ è·å– Node åˆ—è¡¨ï¼Œå¹¶ç»Ÿè®¡ Ready èŠ‚ç‚¹æ•°
// 	nodes, err := client.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
// 	if err != nil {
// 		return nil, fmt.Errorf("æ— æ³•è·å– Node åˆ—è¡¨: %w", err)
// 	}
// 	totalNodes := len(nodes.Items)
// 	readyNodes := 0
// 	for _, node := range nodes.Items {
// 		for _, cond := range node.Status.Conditions {
// 			if cond.Type == "Ready" && cond.Status == "True" {
// 				readyNodes++
// 				break
// 			}
// 		}
// 	}

// 	// 2ï¸âƒ£ è·å–æ‰€æœ‰å‘½åç©ºé—´ä¸‹çš„ Podï¼Œå¹¶ç»Ÿè®¡å¼‚å¸¸ Pod æ•°é‡
// 	pods, err := client.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
// 	if err != nil {
// 		return nil, fmt.Errorf("æ— æ³•è·å– Pod åˆ—è¡¨: %w", err)
// 	}
// 	totalPods := len(pods.Items)
// 	abnormalPods := 0
// 	for _, pod := range pods.Items {
// 		switch pod.Status.Phase {
// 		case "Failed", "Pending", "Unknown":
// 			abnormalPods++
// 		}
// 	}

// 	// 3ï¸âƒ£ è·å– Kubernetes æ§åˆ¶å¹³é¢çš„ç‰ˆæœ¬å·
// 	versionInfo, err := client.Discovery().ServerVersion()
// 	if err != nil {
// 		return nil, fmt.Errorf("æ— æ³•è·å– Kubernetes ç‰ˆæœ¬: %w", err)
// 	}

// 	// 4ï¸âƒ£ æ„å»ºå¹¶è¿”å› ClusterOverview å¯¹è±¡
// 	return &ClusterOverview{
// 		TotalNodes:   totalNodes,
// 		ReadyNodes:   readyNodes,
// 		TotalPods:    totalPods,
// 		AbnormalPods: abnormalPods,
// 		K8sVersion:   versionInfo.GitVersion,
// 		HasMetrics:   utils.HasMetricsServer(),
// 	}, nil
// }
